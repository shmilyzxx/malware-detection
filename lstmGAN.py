import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision.utils import save_image
import os
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
from glob import glob
import wfdb
import numpy as np
from tqdm import tqdm
import torch.nn.functional as F
import os
import cv2
# from torch.distributions.normal import Normal
from torch.autograd import Variable
from torchvision import transforms

class dataset(Dataset):
    def __init__(self):
        super().__init__()
        df=np.loadtxt(r'D:\Users\Downloads\merged_output.csv',str,delimiter=',')
        self .dataset=[]
        self.lab=[]
        c=0
        for i in df:
            da=[]
            try:
                for j in i:
                    da.append(float(j))
                    if float(j)>c:
                        c=float(j)

                if da[7]<0.5:
                    self.dataset.append(np.array(da[:7])/978)
                    self.lab.append(np.array(da[7]))
            except:
                print()
        print(c)




        # self.data_fake = np.loadtxt(r'E:\Project\DTINet\data\Similarity_Matrix_Drugs.txt')[:600]
        #
        # self.data_real = np.loadtxt(r'E:\Project\DTINet\data\mat_drug_drug.txt')[:600]



    def __len__(self):
        return len(self.dataset)
    def __getitem__(self, index):
        data=self.dataset[index]
        return data

class valdataset(Dataset):
    def __init__(self):
        super().__init__()
        df=np.loadtxt(r'D:\Users\Downloads\merged_output.csv',str,delimiter=',')
        self .dataset=[]
        self.lab=[]
        for i in df:
            da=[]
            try:
                for j in i:
                    da.append(float(j))

                # if da[16]<0.5:
                self.dataset.append(np.array(da[:7])/978)
                self.lab.append(np.array(da[7]))
            except:
                print()
        print('---------------------',self.lab)



        # self.data_fake = np.loadtxt(r'E:\Project\DTINet\data\Similarity_Matrix_Drugs.txt')[:600]
        #
        # self.data_real = np.loadtxt(r'E:\Project\DTINet\data\mat_drug_drug.txt')[:600]



    def __len__(self):
        return len(self.dataset)
    def __getitem__(self, index):
        data=self.dataset[index]
        lab_=self.lab[index]

        return data,lab_


class D_Net(nn.Module):
    def __init__(self):
        super(D_Net,self).__init__()
        self.dnet1 = nn.LSTM(input_size=7,
                             hidden_size=1,
                             num_layers=2,
                             batch_first=True,
                             )
    def forward(self, x):
        y,(h_n,h_c) =self.dnet1(x)
        return y
class G_Net(nn.Module):
    def __init__(self):
        super(G_Net, self).__init__()

        self.s=nn.LSTM(input_size=107,
                        hidden_size=7,
                        num_layers=2,
                        batch_first=True,
                      )

    def forward(self, x):
        out,(h_n,h_c) = self.s(x)

        return out

# def calAUC(prob,labels):
#     f = list(zip(prob,labels))
#     rank = [values2 for values1,values2 in sorted(f,key=lambda x:x[0])]
#     rankList = [i+1 for i in range(len(rank)) if rank[i]==1]
#     posNum = 0
#     negNum = 0
#     for i in range(len(labels)):
#         if(labels[i]==1):
#             posNum+=1
#         else:
#             negNum+=1
#     auc = 0
#     auc = (sum(rankList)- (posNum*(posNum+1))/2)/(posNum*negNum)
#     # print(auc)
#     return auc

from sklearn.metrics import precision_score, recall_score, f1_score

if __name__ == '__main__':
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    d_net = D_Net().to(device)
    g_net = G_Net().to(device)
    d_net.train()
    g_net.train()

    loss_fn = nn.BCEWithLogitsLoss()
    loss_fu=nn.MSELoss()
    d_optimizer = torch.optim.Adam(
        d_net.parameters(), lr=0.0002, betas=(0.5, 0.999))
    g_optimizer = torch.optim.Adam(
        g_net.parameters(), lr=0.0002, betas=(0.5, 0.999))

    batch_size=16

    # print(date.shape)
    # lis = []
    # for i in range(445419):
    #     q=date[i,:5]
    #     if q[0]<2 and q[1]<2 and q[2]<2 and q[3]<2 and q[4]<2:
    #         lis.append(date[i].reshape((1,-1)) / np.array([10,50,50,50,50,200,200,200,200,200,200,200,200,200,200,20000,200,200,100000,1000,50,2000,4000,4000]))
    # lis=lis[:444419]
    data_set=dataset()
    val_set=valdataset()
    dataloader = DataLoader(data_set, batch_size=batch_size,
                    shuffle=True,num_workers=0,drop_last=True)
    val_loder=DataLoader(val_set, batch_size=1,
                    shuffle=False,num_workers=0)

    try:
        d_net.load_state_dict(
            torch.load(r"E:\check\dlstm_path"))
        g_net.load_state_dict(
            torch.load(r"E:\check\glstm_path"))
    except:
        print('load_fail')

    loss_fn = nn.BCEWithLogitsLoss()

    pbbox=[]
    bbox=[]
    box=[]

        # vae=VAE().cuda()
    from sklearn.metrics import roc_curve, auc,precision_recall_curve
    if (0):
        for epoch in range(60):
                for i, (img) in enumerate(dataloader):

                    for p in d_net.parameters(): p.data.clamp_(-0.01, 0.01)
                    # img = img / 10
                    # img_real=img_real.reshape(-1,1,1).to(device)
                    # img_fake=img.reshape(-1,1,16).to(device)
                    real_img = img.reshape(-1,1,7).to(device).float()
                    real_label = torch.ones(batch_size) \
                        .view(-1, 1, 1).to(device)
                    fake_label = torch.zeros(batch_size) \
                        .view(-1, 1, 1).to(device)
                    real_out = d_net(real_img)
                    d_loss_real = loss_fn(real_out, real_label)
                    real_scores = real_out
                    z = torch.randn(batch_size, 1, 100).to(device)
                    z=torch.cat((z,real_img),dim=2).float()
                    fake_img = g_net(z)
                    fake_out = d_net(fake_img)
                    d_loss_fake = loss_fn(fake_out, fake_label)
                    fake_scores = fake_out
                    d_loss = (d_loss_real + d_loss_fake)
                    d_optimizer.zero_grad()
                    d_loss.backward()
                    d_optimizer.step()
                    z = torch.randn(batch_size, 1, 100).to(device)
                    z = torch.cat((z, real_img), dim=2).float()
                    fake_img = g_net(z)
                    output = d_net(fake_img)
                    g_loss2 = loss_fn(output, real_label)
                    g_loss=g_loss2
                    g_optimizer.zero_grad()
                    g_loss.backward()
                    g_optimizer.step()
                    if i % 10 == 0:
                        print('encoder', d_loss)
                        print('decoder', g_loss)
                        print('epoch:', epoch)

    if (1):
        # torch.save(d_net.state_dict(), r"E:\Project_\GAN\lstmGAN二分类\LSTMGAN\dlstm_path")
        # torch.save(g_net.state_dict(), r"E:\Project_\GAN\lstmGAN二分类\LSTMGAN\glstm_path")

        d_net.eval()
        g_net.eval()
        acc = 0
        iu=0
        real_=[]
        fake_=[]
        pp=[]
        pr=[]
        y_score=[]
        for q,(img,lab) in enumerate(val_loder):
            # print


            real_img = img.reshape(-1,1,7).float().to(device)
            z = torch.randn(1, 1, 100).to(device)

            z = torch.cat((z, real_img), dim=2).float()
            fake_img = g_net(z)

            x=torch.mean(real_img-fake_img)
            print(x)
            y_score.append(x.item())
            if x<=0.03:
                fake_.append(0.)
            else:
                fake_.append(1.)
            real_.append(int(lab.item()))



        for i,(x) in enumerate(real_):
            f=fake_
            iu+=1
            if x==f[i] or int(f[i])==0:
                acc+=1
        # fpr,tpr,t=roc_curve(fake_,real_)
        # print(tpr)
        # # auc1=calAUC(fake_,real_)
        # roc_auc=auc(fpr,tpr)
        # lw = 2
        # plt.figure(figsize=(10, 10))
        # plt.plot(fpr, tpr, color='darkorange',
        #          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)  ###假正率为横坐标，真正率为纵坐标做曲线
        # plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
        # plt.xlim([0.0, 1.0])
        # plt.ylim([0.0, 1.05])
        # plt.xlabel('False Positive Rate')
        # plt.ylabel('True Positive Rate')
        # plt.title('Receiver operating characteristic example')
        # plt.legend(loc="lower right")
        # plt.savefig('./roc.jpg')
        # plt.clf()
        # import pdb
        # pdb.set_trace()

        from sklearn.utils.multiclass import type_of_target

        # p, r, thresholds = precision_recall_curve(real_, fake_)
        # f1score = 2*(p * r)/(p + r)
        tp=0
        fp=0
        fn=0

        for p,(re) in enumerate(fake_):

            if int(re)==int(real_[p]):
                tp+=1
            if int(re)==1 and int(real_[p])==0:
                fp+=1
            if int(re)==0 and int(real_[p])==1:
                fn+=1
        p=tp/(tp+fp)
        r=tp/(iu-fp)

        print('p:', tp/(tp+fp))
        print('r:', tp/(iu-fp))
        f1score = 2 * (p * r) / (p + r)
        print('f1:', f1score)


        # plt.plot(np.array(r),np.array(p))
        # plt.title('PR')
        # plt.savefig('./pr.jpg')
        # print('auc:',roc_auc)
        #
        print('acc:',acc/iu)
        g_net.train()
        d_net.train()



                # pbbox.append(g_loss.cpu().detach().numpy())
                # # sub_axix = filter(lambda x: x % 200 == 0, pbbox)
                # plt.plot(pbbox, color='blue')
                # # plt.legend()
                # plt.title('g_loss')
                # plt.ylabel('g_loss')
                # plt.pause(0.001)
                # bbox.append(g_loss.cpu().detach().numpy())
                # # su_axix = filter(lambda x: x % 200 == 0, bbox)
                # plt.plot(bbox, color='blue', )
                # # plt.legend()
                # plt.text(0, 1.58, 'blue-g_net', size=15)
                # plt.text(0, 1.67, 'red-d_net', size=15)
                # plt.title('loss')
                # plt.ylabel('loss')
                # plt.pause(0.001)
                #
                # box.append(loss_dis.cpu().detach().numpy())
                # plt.plot(box, color='purple', )
                # # plt.legend()
                # plt.text(0, 1.58, 'blue-g_net', size=15)
                # plt.text(0, 1.67, 'red-d_net', size=15)
                # plt.title('loss')
                # plt.ylabel('loss')
                # plt.pause(0.001)



